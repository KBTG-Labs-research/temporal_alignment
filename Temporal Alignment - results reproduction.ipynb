{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df09ca08",
   "metadata": {},
   "source": [
    "# Reproduction Notebook for\n",
    "# \"Temporal Alignment: Evaluation for Temporal Relation Extraction\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aecef58",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You will need two Python environment in order to perform complete reproduction of all experiments in our paper.\n",
    "\n",
    "1. Python 3.8+ - the environment you will be using to run this Jupyter notebook\n",
    "2. Python 2.7 - an additional Python environment we will use to run some commands to utilize old code\n",
    "\n",
    "We will be doing as much work as possible in Python 3.8+ (within this notebook) and execute some command in a Python 2.7 env from time to time.\n",
    "\n",
    "### Python 3.8+ Env\n",
    "\n",
    "This notebook is tested on Python `3.8.16`. However it should work with any Python version `3.8` or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01398a57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:46.555226Z",
     "start_time": "2024-01-12T07:43:46.396126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.16\r\n"
     ]
    }
   ],
   "source": [
    "# check Python\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ccacbe",
   "metadata": {},
   "source": [
    "Install dependencies\n",
    "\n",
    "The only dependency we need is `tieval`. The notebook is tested with `tieval==0.1.1`, so that is what we will be installing.\n",
    "\n",
    "Although installation should be simple as\n",
    "\n",
    "```\n",
    "pip install tieval==0.1.1\n",
    "```\n",
    "\n",
    "but we have found better success with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aacdbde4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:46.558792Z",
     "start_time": "2024-01-12T07:43:46.556846Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install torch==1.11.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "# !pip install --default-timeout=300 tieval==0.1.1 allennlp==2.9.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc4aa1",
   "metadata": {},
   "source": [
    "Make our evaluation library importable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb71e118",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:46.591499Z",
     "start_time": "2024-01-12T07:43:46.559800Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt -r dev-requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c34ed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:46.625730Z",
     "start_time": "2024-01-12T07:43:46.592567Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(\"lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931ada0",
   "metadata": {},
   "source": [
    "###  Python 2.7\n",
    "\n",
    "This environment is used for running UzZaman's Temporal Awareness implementation.\n",
    "\n",
    "#### Setting up a clean Python 2.7 env with docker.\n",
    "\n",
    "Pull the `python:2.7.18` docker image\n",
    "```\n",
    "docker pull python:2.7.18\n",
    "```\n",
    "\n",
    "Run the docker image is as a detached container named \"temporal-py27\" with this repository mounted.  \n",
    "Be sure to replace `/home/user/dev/repo` with an absolute path to this repo on your machine.\n",
    "```\n",
    "docker run -d --name temporal-py27 -v /home/user/dev/repo:/workspace -u 1000:1000 python:2.7.18 tail -f /dev/null\n",
    "```\n",
    "\n",
    "To shell into the \"temporal-py27\" container, use the following command\n",
    "```\n",
    "docker exec -it temporal-py27 bash\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85854f70",
   "metadata": {},
   "source": [
    "## Experiments on TempEval3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b096a88d",
   "metadata": {},
   "source": [
    "### Prep data\n",
    "\n",
    "NOTE: since tieval store the tlinks in a `set`, iterating over across multiple runs (Python restarts) leads to inconsistent annotation order. This leads to different outcome for some evaluation metrics.\n",
    "\n",
    "Sort annotation entries for consistent evaluation. Since order of original dataset is lost, we will sort in order of their position in the text.\n",
    "\n",
    "This would not affect greedy algorithms removing closure violations since annotations in TE3 does not have any closure violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b46398af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:47.885366Z",
     "start_time": "2024-01-12T07:43:46.627357Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275 [00:00<00:00, 624.48it/s]\n",
      "100%|██████████| 275/275 [00:00<00:00, 571.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from tieval import datasets\n",
    "\n",
    "# `te3_ordered` will be sorted\n",
    "te3_ordered = datasets.read(\"tempeval_3\")\n",
    "# `te3` kept as is\n",
    "te3 = datasets.read(\"tempeval_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed41e850",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:47.889147Z",
     "start_time": "2024-01-12T07:43:47.886393Z"
    }
   },
   "outputs": [],
   "source": [
    "def tlink_pos(tlink):\n",
    "    \"get position of a tlink\"\n",
    "    source_offset = tlink.source.offsets\n",
    "    target_offset = tlink.target.offsets\n",
    "    if source_offset is None:\n",
    "        assert tlink.source.id == \"t0\"\n",
    "        source_offset = (0, 0)\n",
    "    if target_offset is None:\n",
    "        assert tlink.target.id == \"t0\"\n",
    "        target_offset = (0, 0)\n",
    "        \n",
    "    if source_offset < target_offset:\n",
    "        return source_offset, target_offset\n",
    "    else:\n",
    "        return target_offset, source_offset\n",
    "\n",
    "te3_test_tlinks = {}\n",
    "\n",
    "for entry in te3_ordered.test:\n",
    "    tlinks = sorted(entry.tlinks, key=tlink_pos)\n",
    "    te3_test_tlinks[entry.name] = tlinks\n",
    "    entry.tlinks = tlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bee82b8",
   "metadata": {},
   "source": [
    "### Inference with CogComp2\n",
    "\n",
    "Load model and perform inference on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e7e6de",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56fe968c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:47.922226Z",
     "start_time": "2024-01-12T07:43:47.889903Z"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c5cee20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:47.944692Z",
     "start_time": "2024-01-12T07:43:47.923267Z"
    }
   },
   "outputs": [],
   "source": [
    "# from tieval.models.classification.temporal_relation import CogCompTime2\n",
    "\n",
    "# model = CogCompTime2()\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db39915",
   "metadata": {},
   "source": [
    "#### Inference & Save\n",
    "\n",
    "We perform three inference passes since `CogCompTime2` is not deterministic. The body of the paper reports on the 1st run. Runs 2nd and 3rd are reported in the appendix.\n",
    "\n",
    "Readers looking to reproduce the exact evaluation should load the inference output. We have inline values you should get for the 1st inference in the notebook. For values in the 2nd and 3rd inference, please refer to the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "913f2016",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:47.971325Z",
     "start_time": "2024-01-12T07:43:47.945449Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a31a90e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:47.987025Z",
     "start_time": "2024-01-12T07:43:47.972364Z"
    }
   },
   "outputs": [],
   "source": [
    "# prediction = model.predict(te3_ordered.test)\n",
    "# with open(\"output/inference-1.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(prediction, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2053665b",
   "metadata": {},
   "source": [
    "Two additional runs for appendix section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3625ce84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.003736Z",
     "start_time": "2024-01-12T07:43:47.987793Z"
    }
   },
   "outputs": [],
   "source": [
    "# prediction = model.predict(te3.test)\n",
    "# with open(\"output/inference-2.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(prediction, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a48629b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.023059Z",
     "start_time": "2024-01-12T07:43:48.005051Z"
    }
   },
   "outputs": [],
   "source": [
    "# prediction = model.predict(te3.test)\n",
    "# with open(\"output/inference-3.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(prediction, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb036418",
   "metadata": {},
   "source": [
    "#### Load Inference\n",
    "\n",
    "Loading inference is prefered since CogCompTime2 is not deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7341e257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.039117Z",
     "start_time": "2024-01-12T07:43:48.024154Z"
    }
   },
   "outputs": [],
   "source": [
    "infer_pass = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81a27e13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.055862Z",
     "start_time": "2024-01-12T07:43:48.042003Z"
    }
   },
   "outputs": [],
   "source": [
    "# infer_pass = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e30670e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.079297Z",
     "start_time": "2024-01-12T07:43:48.056939Z"
    }
   },
   "outputs": [],
   "source": [
    "# infer_pass = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c46c44f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.103527Z",
     "start_time": "2024-01-12T07:43:48.080545Z"
    }
   },
   "outputs": [],
   "source": [
    "infer_path = f\"output/inference-{infer_pass}.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85606370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.130785Z",
     "start_time": "2024-01-12T07:43:48.104641Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(infer_path, \"rb\") as file:\n",
    "    prediction = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce9552",
   "metadata": {},
   "source": [
    "### UzZaman - Temporal Awareness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e420c6",
   "metadata": {},
   "source": [
    "#### Prep\n",
    "\n",
    "Prepare files as input for [UzZaman - Temporal Awareness evaluator](https://github.com/naushadzaman/tempeval3_toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ef00cee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.153111Z",
     "start_time": "2024-01-12T07:43:48.131798Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ref_path = Path(f\"output/tempeval3_toolkit/ref-{infer_pass}\")\n",
    "pred_path = Path(f\"output/tempeval3_toolkit/pred-{infer_pass}\")\n",
    "\n",
    "os.makedirs(ref_path, exist_ok=True)\n",
    "os.makedirs(pred_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abd0901",
   "metadata": {},
   "source": [
    "Tooling for rendering TLinks to TimeML format\n",
    "\n",
    "Relations not in `tlink_rel_types` will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e2eabdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.232926Z",
     "start_time": "2024-01-12T07:43:48.154416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tlink_rel_types: {'BEFORE', 'ENDS', 'AFTER', 'DURING', 'IAFTER', 'IBEFORE', 'BEGINS', 'INCLUDES', 'SIMULTANEOUS', 'BEGUN_BY', 'ENDED_BY', 'DURING_INV', 'IDENTITY', 'IS_INCLUDED'}\n"
     ]
    }
   ],
   "source": [
    "from tieval import entities\n",
    "\n",
    "from temporal_extract.rep.raw_timeml import tlink_rel_types\n",
    "\n",
    "\n",
    "print(\"tlink_rel_types:\", tlink_rel_types)\n",
    "\n",
    "def render_timeml_tlinks(tlinks):\n",
    "    \"\"\"Render tieval tlinks into UzZaman\n",
    "    Temporal Awareness TimeML TLINKS compatible file\"\"\"\n",
    "    \n",
    "    buffer = \"\"\n",
    "    for tlink in tlinks:\n",
    "        if isinstance(tlink.source, entities.Event):\n",
    "            src_type = \"eventInstanceID\"\n",
    "            src_id = tlink.source.eiid\n",
    "        else:\n",
    "            src_type = \"timeID\"\n",
    "            src_id = tlink.source.id\n",
    "        if isinstance(tlink.target, entities.Event):\n",
    "            tgt_type = \"relatedToEventInstance\"\n",
    "            tgt_id = tlink.target.eiid\n",
    "        else:\n",
    "            tgt_type = \"relatedToTime\"\n",
    "            tgt_id = tlink.target.id\n",
    "            \n",
    "        if tlink.relation.interval not in tlink_rel_types:\n",
    "            # if relation not in `tlink_rel_types` and report\n",
    "            print(\"removed relation\", tlink.relation.interval)\n",
    "            continue\n",
    "            \n",
    "        buffer += f'<TLINK lid=\"{tlink.id}\" {src_type}=\"{src_id}\" relType=\"{tlink.relation.interval}\" {tgt_type}=\"{tgt_id}\"/>\\n'\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f66606f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.239182Z",
     "start_time": "2024-01-12T07:43:48.233778Z"
    }
   },
   "outputs": [],
   "source": [
    "for name, tlinks in te3_test_tlinks.items():\n",
    "    with open(ref_path / f\"{name}.tml\", \"w\") as file:\n",
    "        file.write(render_timeml_tlinks(tlinks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2668dad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.280792Z",
     "start_time": "2024-01-12T07:43:48.239864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n",
      "removed relation VAGUE\n"
     ]
    }
   ],
   "source": [
    "for name, tlinks in prediction.items():\n",
    "    with open(pred_path / f\"{name}.tml\", \"w\") as file:\n",
    "        file.write(render_timeml_tlinks(tlinks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d06b9f",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1321f6c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T08:21:16.458463Z",
     "start_time": "2024-01-10T08:21:16.454926Z"
    }
   },
   "source": [
    "Git clone https://github.com/naushadzaman/tempeval3_toolkit\n",
    "\n",
    "Use the following code to generate commands to evaluate using UzZaman Temporal Awareness implementation in a Python 2.7 env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e35cf1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.307712Z",
     "start_time": "2024-01-12T07:43:48.281758Z"
    }
   },
   "outputs": [],
   "source": [
    "path_to_tempeval3_toolkit_repo = \"repos/tempeval3_toolkit\"\n",
    "path_to_this_repo = \"temporal_alignment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cff80665",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.332365Z",
     "start_time": "2024-01-12T07:43:48.308714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python repos/tempeval3_toolkit/evaluation-relations/temporal_evaluation.py \\\n",
      "    temporal_alignment/output/tempeval3_toolkit/ref-1 \\\n",
      "    temporal_alignment/output/tempeval3_toolkit/pred-1 \\\n",
      "    1 > temporal_alignment/output/tempeval3_toolkit/eval-ta-1.txt\n",
      "\n",
      "python repos/tempeval3_toolkit/evaluation-relations/temporal_evaluation.py \\\n",
      "    temporal_alignment/output/tempeval3_toolkit/ref-1 \\\n",
      "    temporal_alignment/output/tempeval3_toolkit/pred-1 \\\n",
      "    1 acl11 > temporal_alignment/output/tempeval3_toolkit/eval-ta-1-acl11.txt\n",
      "    \n",
      "python repos/tempeval3_toolkit/evaluation-relations/temporal_evaluation.py \\\n",
      "    temporal_alignment/output/tempeval3_toolkit/ref-1 \\\n",
      "    temporal_alignment/output/tempeval3_toolkit/pred-1 \\\n",
      "    1 implicit_in_recall > temporal_alignment/output/tempeval3_toolkit/eval-ta-1-implicit_in_recall.txt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ta_script_path = f\"{path_to_tempeval3_toolkit_repo}/evaluation-relations/temporal_evaluation.py\"\n",
    "output_prefix = f\"output/tempeval3_toolkit/eval-ta-{infer_pass}\"\n",
    "print(f\"\"\"python {ta_script_path} \\\\\n",
    "    {path_to_this_repo}/{ref_path} \\\\\n",
    "    {path_to_this_repo}/{pred_path} \\\\\n",
    "    1 > {path_to_this_repo}/{output_prefix}.txt\n",
    "\n",
    "python {ta_script_path} \\\\\n",
    "    {path_to_this_repo}/{ref_path} \\\\\n",
    "    {path_to_this_repo}/{pred_path} \\\\\n",
    "    1 acl11 > {path_to_this_repo}/{output_prefix}-acl11.txt\n",
    "    \n",
    "python {ta_script_path} \\\\\n",
    "    {path_to_this_repo}/{ref_path} \\\\\n",
    "    {path_to_this_repo}/{pred_path} \\\\\n",
    "    1 implicit_in_recall > {path_to_this_repo}/{output_prefix}-implicit_in_recall.txt\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8aaf51ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.366819Z",
     "start_time": "2024-01-12T07:43:48.333663Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_scores(path):\n",
    "    with open(path) as file:\n",
    "        lines = file.readlines()\n",
    "    # print tail of evaluation file\n",
    "    print(\"\".join(lines[-4:-1]))\n",
    "    \n",
    "    # format scores as Latex Table\n",
    "    f1, p, r = lines[-3].split(\"\\t\")[2:5]\n",
    "    f1 = float(f1) / 100\n",
    "    p = float(p) / 100\n",
    "    r = float(r) / 100\n",
    "    return f\"{p:.4f} & {r:.4f} & {f1:.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e66d8f81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.392567Z",
     "start_time": "2024-01-12T07:43:48.368099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Score\tF1\tP\tR\n",
      "\t\t39.8088\t40.2537\t39.3736\t\n",
      "Overall Temporal Awareness Score (F1 score): 39.8088\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.4025 & 0.3937 & 0.3981'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_scores(f\"output/tempeval3_toolkit/eval-ta-{infer_pass}.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcccf1c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T08:47:46.792270Z",
     "start_time": "2024-01-10T08:47:46.789764Z"
    }
   },
   "source": [
    "should be\n",
    "```\n",
    "Temporal Score\tF1\tP\tR\n",
    "\t\t39.8088\t40.2537\t39.3736\t\n",
    "Overall Temporal Awareness Score (F1 score): 39.8088\n",
    "\n",
    "'0.4025 & 0.3937 & 0.3981'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22668f65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.417097Z",
     "start_time": "2024-01-12T07:43:48.393744Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Score\tF1\tP\tR\n",
      "\t\t40.6634\t41.1894\t40.1507\t\n",
      "Overall Temporal Awareness Score (F1 score): 40.6634\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.4119 & 0.4015 & 0.4066'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_scores(f\"output/tempeval3_toolkit/eval-ta-{infer_pass}-acl11.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf9c47c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T08:47:46.792270Z",
     "start_time": "2024-01-10T08:47:46.789764Z"
    }
   },
   "source": [
    "should be\n",
    "```\n",
    "Temporal Score\tF1\tP\tR\n",
    "\t\t40.6634\t41.1894\t40.1507\t\n",
    "Overall Temporal Awareness Score (F1 score): 40.6634\n",
    "\n",
    "'0.4119 & 0.4015 & 0.4066'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f63d960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.440380Z",
     "start_time": "2024-01-12T07:43:48.418314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Score\tF1\tP\tR\n",
      "\t\t39.8094\t40.2537\t39.3747\t\n",
      "Overall Temporal Awareness Score (F1 score): 39.8094\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.4025 & 0.3937 & 0.3981'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_scores(f\"output/tempeval3_toolkit/eval-ta-{infer_pass}-implicit_in_recall.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1ac08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T08:47:46.792270Z",
     "start_time": "2024-01-10T08:47:46.789764Z"
    }
   },
   "source": [
    "should be\n",
    "```\n",
    "Temporal Score\tF1\tP\tR\n",
    "\t\t39.8094\t40.2537\t39.3747\t\n",
    "Overall Temporal Awareness Score (F1 score): 39.8094\n",
    "\n",
    "'0.4025 & 0.3937 & 0.3981'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0bf81c",
   "metadata": {},
   "source": [
    "### TiEval - Temporal Awareness\n",
    "\n",
    "Note: we are not using `evaluate.temporal_awareness` since it would produce scores on a per document basis.\n",
    "However, we want to evaluate on the whole dataset.\n",
    "\n",
    "We will use `te3` since `tieval.evaluate.temporal_recall` and `tieval.evaluate.temporal_precision` operate on `set` instead of `list`.\n",
    "\n",
    "NOTE: since tieval Temporal Awareness implementation is also affect ordering, the numerical results will not be consistent across restarts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f73e453",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.817475Z",
     "start_time": "2024-01-12T07:43:48.441317Z"
    }
   },
   "outputs": [],
   "source": [
    "from tieval import evaluate\n",
    "\n",
    "\n",
    "ref_total = 0\n",
    "ref_correct = 0\n",
    "pred_total = 0\n",
    "pred_correct = 0\n",
    "\n",
    "for doc in te3.test:\n",
    "\n",
    "    ref = doc.tlinks\n",
    "    pred = set(prediction[doc.name])\n",
    "\n",
    "    cor, total = evaluate.temporal_recall(pred, ref)\n",
    "    ref_total += total\n",
    "    ref_correct += cor\n",
    "    cor, total = evaluate.temporal_precision(pred, ref)\n",
    "    pred_total += total\n",
    "    pred_correct += cor\n",
    "    \n",
    "recall = ref_correct / ref_total\n",
    "precision = pred_correct / pred_total\n",
    "\n",
    "temporal_awareness = 2 * recall * precision / (recall + precision) if (recall + precision) else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "332bc07c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.820282Z",
     "start_time": "2024-01-12T07:43:48.818281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3983 & 0.3940 & 0.3961'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{precision:.4f} & {recall:.4f} & {temporal_awareness:.4f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dd3705",
   "metadata": {},
   "source": [
    "should be **approximately**\n",
    "\n",
    "`'0.3983 & 0.3940 & 0.3961'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd326fd",
   "metadata": {},
   "source": [
    "### Ours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e47a4",
   "metadata": {},
   "source": [
    "Tooling for handling TiEval data and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cda026b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.859631Z",
     "start_time": "2024-01-12T07:43:48.820935Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "from tieval import entities\n",
    "\n",
    "\n",
    "def get_id(ins: Union[entities.Event, entities.Timex]) -> str:\n",
    "    \"get ID from a TiEval entity\"\n",
    "    if isinstance(ins, entities.Event):\n",
    "        return ins.eiid\n",
    "    elif isinstance(ins, entities.Timex):\n",
    "        return ins.id\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def tieval_rels_2_te_rels(rels) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Convert TiEval relations format into our temporal entitie relations format\n",
    "    which is an array of Tuple[source_id, target_id, relation]\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (get_id(rel.source), get_id(rel.target), rel.relation.interval)\n",
    "        for rel in rels\n",
    "        # remove \"VAGUE\" relation\n",
    "        if rel.relation.interval != \"VAGUE\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d98db974",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:48.894592Z",
     "start_time": "2024-01-12T07:43:48.860626Z"
    }
   },
   "outputs": [],
   "source": [
    "from temporal_extract.rep.timeml_graph import TmlGraph\n",
    "from temporal_extract.scorer.base import f_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be341a",
   "metadata": {},
   "source": [
    "#### No prediction post-processing\n",
    "\n",
    "Prediction outputs are evaluated as-is, without conflict removal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d5dc08",
   "metadata": {},
   "source": [
    "##### Temporal Awareness (Our implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be03b386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:49.519217Z",
     "start_time": "2024-01-12T07:43:48.895551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] `TemporalAwarenessScorer` is for score reproduction only. **DO NOT USE** for acutal evaluation!\n",
      "\n",
      "    For accurate evaluation of temporal relations between system-prediction and ref-standard please use TemporalPointAlignmentScorer or TemporalEntityAlignmentScorer instead.\n",
      "\n",
      "    The TemporalAwarenessScorer computes \"Temporal Awarness score\" accoring to the paper titled \"UzZaman, Naushad. Interpreting the temporal aspects of language. University of Rochester, 2012.\" and the reference implementation from https://github.com/naushadzaman/tempeval3_toolkit.\n",
      "\n",
      "    This reimplementation also inherit all of the quirks of `tempeval3_toolkit` not discussed in the paper, while NOT inheriting the issues present in the original closure graph implementation.\n",
      "\n",
      "    quirks include\n",
      "     * greedy removal of closure violation\n",
      "     * matching relations with removed violation causing relations\n",
      "\n",
      "    This leads to this implementation of TemporalAwareness favoring outputs with closure violation, which is not ideal.\n",
      "    \n",
      "0.3979 & 0.3895 & 0.3937\n",
      "0.3979 & 0.3895 & 0.3937\n",
      "0.3979 & 0.3895 & 0.3937\n",
      "0.3979 & 0.3895 & 0.3937\n",
      "0.3979 & 0.3895 & 0.3937\n"
     ]
    }
   ],
   "source": [
    "from temporal_extract.scorer.temporal_awareness import TemporalAwarenessScorer\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    ta_scorer = TemporalAwarenessScorer(suppress_warning=i!=0)\n",
    "\n",
    "    doc_eval = {}\n",
    "\n",
    "    for idx, doc in enumerate(te3_ordered.test):\n",
    "        # iterate over each document\n",
    "\n",
    "        ref = doc.tlinks\n",
    "        pred = prediction[doc.name]\n",
    "\n",
    "        # convert reference relations\n",
    "        ref_te_rels = tieval_rels_2_te_rels(ref)\n",
    "        # convert predicted relations\n",
    "        pred_te_rels = tieval_rels_2_te_rels(pred)\n",
    "\n",
    "        # evaluate the document\n",
    "        doc_eval[doc.name] = ta_scorer.evaluate_relations(ref_te_rels, pred_te_rels)\n",
    "\n",
    "    # summarize evaluation of the dataset (all documents)\n",
    "    res = ta_scorer.summarize()\n",
    "\n",
    "    precision = res[\"precision\"]\n",
    "    recall = res[\"recall\"]\n",
    "    temporal_awareness = res[\"fscore\"]\n",
    "    print(f\"{precision:.4f} & {recall:.4f} & {temporal_awareness:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab94178",
   "metadata": {},
   "source": [
    "should be\n",
    "```\n",
    "'0.3979 & 0.3895 & 0.3937'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6edaf85",
   "metadata": {},
   "source": [
    "##### Temporal Entity Alignment\n",
    "\n",
    "We are running the evaluation metric 5 times with random order annotation ordering to demonstrate robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8c9ec58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:50.798717Z",
     "start_time": "2024-01-12T07:43:49.520076Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Entity Alignment\n",
      "Run: 0\n",
      "0.3651 & 0.3321 & 0.3478\n",
      "doc affected by violations: 4\n",
      "\n",
      "Run: 1\n",
      "0.3651 & 0.3321 & 0.3478\n",
      "doc affected by violations: 4\n",
      "\n",
      "Run: 2\n",
      "0.3651 & 0.3321 & 0.3478\n",
      "doc affected by violations: 4\n",
      "\n",
      "Run: 3\n",
      "0.3651 & 0.3321 & 0.3478\n",
      "doc affected by violations: 4\n",
      "\n",
      "Run: 4\n",
      "0.3651 & 0.3321 & 0.3478\n",
      "doc affected by violations: 4\n",
      "\n",
      "total docs: 20\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from temporal_extract.scorer.temporal_alignment import TemporalEntityAlignmentScorer\n",
    "\n",
    "\n",
    "print(\"Temporal Entity Alignment\")\n",
    "\n",
    "for i in range(5):\n",
    "    tea_scorer = TemporalEntityAlignmentScorer()\n",
    "\n",
    "    # store evaluation of each document (used for futher analysis)\n",
    "    doc_eval = {}\n",
    "\n",
    "    for idx, doc in enumerate(te3_ordered.test):\n",
    "        # iterate over each document\n",
    "\n",
    "        # shuffle ref and pred to demonstrate our evaluation robustness\n",
    "        ref = doc.tlinks\n",
    "        pred = prediction[doc.name]\n",
    "\n",
    "        # convert reference relations\n",
    "        ref_te_rels = tieval_rels_2_te_rels(ref)\n",
    "        # convert predicted relations\n",
    "        pred_te_rels = tieval_rels_2_te_rels(pred)\n",
    "        \n",
    "        # shuffle ref and pred to demonstrate our evaluation robustness\n",
    "        # random.shuffle(ref_te_rels)\n",
    "        # random.shuffle(pred_te_rels)\n",
    "\n",
    "        # evaluate the document\n",
    "        doc_eval[doc.name] = tea_scorer.evaluate_relations(ref_te_rels, pred_te_rels)\n",
    "\n",
    "    # summarize evaluation of the dataset (all documents)\n",
    "    res = tea_scorer.summarize()\n",
    "\n",
    "    precision = res[\"precision\"]\n",
    "    recall = res[\"recall\"]\n",
    "    temporal_awareness = res[\"fscore\"]\n",
    "    \n",
    "    print(\"Run:\", i)\n",
    "    print(f\"{precision:.4f} & {recall:.4f} & {temporal_awareness:.4f}\")\n",
    "    print(f\"doc affected by violations: {tea_scorer.model_doc_violation_count}\")\n",
    "    print()\n",
    "    \n",
    "print(f\"total docs: {len(te3.test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e44600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T09:58:29.043889Z",
     "start_time": "2024-01-06T09:58:29.030198Z"
    }
   },
   "source": [
    "every run should be **approximately**\n",
    "\n",
    "```\n",
    "0.3651 & 0.3321 & 0.3478\n",
    "0.3651 & 0.3309 & 0.3471\n",
    "doc affected by violations: 4\n",
    "\n",
    "total docs: 20\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a246c97",
   "metadata": {},
   "source": [
    "##### Temporal Point Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92c21bf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:51.188042Z",
     "start_time": "2024-01-12T07:43:50.799733Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Point Alignment\n",
      "Run: 0\n",
      "0.3584 & 0.3834 & 0.3705\n",
      "potins affected by violations: 26\n",
      "total points: 823\n",
      "\n",
      "Run: 1\n",
      "0.3584 & 0.3834 & 0.3705\n",
      "potins affected by violations: 26\n",
      "total points: 823\n",
      "\n",
      "Run: 2\n",
      "0.3584 & 0.3834 & 0.3705\n",
      "potins affected by violations: 26\n",
      "total points: 823\n",
      "\n",
      "Run: 3\n",
      "0.3584 & 0.3834 & 0.3705\n",
      "potins affected by violations: 26\n",
      "total points: 823\n",
      "\n",
      "Run: 4\n",
      "0.3584 & 0.3834 & 0.3705\n",
      "potins affected by violations: 26\n",
      "total points: 823\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from temporal_extract.scorer.temporal_alignment import TemporalPointAlignmentScorer\n",
    "\n",
    "\n",
    "print(\"Temporal Point Alignment\")\n",
    "\n",
    "for i in range(5):\n",
    "    tpa_scorer = TemporalPointAlignmentScorer()\n",
    "\n",
    "    # store evaluation of each document (used for futher analysis)\n",
    "    doc_eval = {}\n",
    "\n",
    "    for idx, doc in enumerate(te3_ordered.test):\n",
    "        # iterate over each document\n",
    "\n",
    "        # shuffle ref and pred to demonstrate our evaluation robustness\n",
    "        ref = doc.tlinks\n",
    "        pred = prediction[doc.name]\n",
    "\n",
    "        # convert reference relations\n",
    "        ref_te_rels = tieval_rels_2_te_rels(ref)\n",
    "        # convert predicted relations\n",
    "        pred_te_rels = tieval_rels_2_te_rels(pred)\n",
    "        \n",
    "        # shuffle ref and pred to demonstrate our evaluation robustness\n",
    "        random.shuffle(ref_te_rels)\n",
    "        random.shuffle(pred_te_rels)\n",
    "\n",
    "        # evaluate the document\n",
    "        doc_eval[doc.name] = tpa_scorer.evaluate_relations(ref_te_rels, pred_te_rels)\n",
    "\n",
    "    # summarize evaluation of the dataset (all documents)\n",
    "    res = tpa_scorer.summarize()\n",
    "\n",
    "    precision = res[\"precision\"]\n",
    "    recall = res[\"recall\"]\n",
    "    temporal_awareness = res[\"fscore\"]\n",
    "    \n",
    "    print(\"Run:\", i)\n",
    "    print(f\"{precision:.4f} & {recall:.4f} & {temporal_awareness:.4f}\")\n",
    "    print(f\"potins affected by violations: {tpa_scorer.pt_affected}\")\n",
    "    print(f\"total points: {tpa_scorer.pt_total}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0bf2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T09:58:38.683263Z",
     "start_time": "2024-01-06T09:58:38.674764Z"
    }
   },
   "source": [
    "every run should be\n",
    "\n",
    "```\n",
    "0.3584 & 0.3834 & 0.3705\n",
    "potins affected by violations: 26\n",
    "total points: 823\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892f968",
   "metadata": {},
   "source": [
    "#### Greedily Violation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "783ef812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:51.287728Z",
     "start_time": "2024-01-12T07:43:51.188985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closure violation 1 in WSJ_20130321_1145\n",
      "closure violation 1 in WSJ_20130318_731\n",
      "closure violation 1 in nyt_20130321_women_senate\n",
      "closure violation 2 in WSJ_20130322_804\n",
      "\n",
      "Total closure violation causing entity-relations: 5\n",
      "Total entity-relations: 908\n",
      "Total core entity-relations: 808\n"
     ]
    }
   ],
   "source": [
    "core_ent_prediction = {}\n",
    "ent_closure_violation_count = 0\n",
    "ent_count = 0\n",
    "core_ent_count = 0\n",
    "\n",
    "for idx, doc in enumerate(te3.test):\n",
    "    # filters out closure violation greedily\n",
    "    \n",
    "    # convert tieval to our format\n",
    "    pred_te_rels = tieval_rels_2_te_rels(prediction[doc.name])\n",
    "    \n",
    "    # create prediciton graph for doc\n",
    "    ent_count += len(pred_te_rels)\n",
    "    pred_graph = TmlGraph()\n",
    "    # safely add relation to graph (in-order) without incurring closure violation\n",
    "    _, _, closure_violation = pred_graph.safe_add_relations(pred_te_rels)\n",
    "    # collect violation count\n",
    "    if len(closure_violation) > 0:\n",
    "        print(\"closure violation\", len(closure_violation), \"in\", doc.name)\n",
    "    ent_closure_violation_count += len(closure_violation)\n",
    "    \n",
    "    # create a new set of entities-relation which encompass all relation data of the violation-free graph\n",
    "    core_ent = pred_graph.compute_core_entity_relations()\n",
    "    core_ent_prediction[doc.name] = core_ent\n",
    "    core_ent_count += len(core_ent)\n",
    "    \n",
    "\n",
    "print(f\"\\nTotal closure violation causing entity-relations: {ent_closure_violation_count}\")\n",
    "print(f\"Total entity-relations: {ent_count}\")\n",
    "print(f\"Total core entity-relations: {core_ent_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0e615",
   "metadata": {},
   "source": [
    "##### Temporal Awareness (Our implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a58e9409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T10:08:37.364855Z",
     "start_time": "2024-01-12T10:08:36.804061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] `TemporalAwarenessScorer` is for score reproduction only. **DO NOT USE** for acutal evaluation!\n",
      "\n",
      "    For accurate evaluation of temporal relations between system-prediction and ref-standard please use TemporalPointAlignmentScorer or TemporalEntityAlignmentScorer instead.\n",
      "\n",
      "    The TemporalAwarenessScorer computes \"Temporal Awarness score\" accoring to the paper titled \"UzZaman, Naushad. Interpreting the temporal aspects of language. University of Rochester, 2012.\" and the reference implementation from https://github.com/naushadzaman/tempeval3_toolkit.\n",
      "\n",
      "    This reimplementation also inherit all of the quirks of `tempeval3_toolkit` not discussed in the paper, while NOT inheriting the issues present in the original closure graph implementation.\n",
      "\n",
      "    quirks include\n",
      "     * greedy removal of closure violation\n",
      "     * matching relations with removed violation causing relations\n",
      "\n",
      "    This leads to this implementation of TemporalAwareness favoring outputs with closure violation, which is not ideal.\n",
      "    \n",
      "0.3923 & 0.3895 & 0.3909\n",
      "0.3923 & 0.3895 & 0.3909\n",
      "0.3923 & 0.3895 & 0.3909\n",
      "0.3923 & 0.3895 & 0.3909\n",
      "0.3923 & 0.3895 & 0.3909\n"
     ]
    }
   ],
   "source": [
    "from temporal_extract.scorer.temporal_awareness import TemporalAwarenessScorer\n",
    "\n",
    "for i in range(5):\n",
    "    ta_scorer = TemporalAwarenessScorer(suppress_warning=i!=0)\n",
    "\n",
    "    doc_eval = {}\n",
    "\n",
    "    for idx, doc in enumerate(te3_ordered.test):\n",
    "        # iterate over each document\n",
    "\n",
    "        # convert reference relations\n",
    "        ref_te_rels = tieval_rels_2_te_rels(doc.tlinks)\n",
    "\n",
    "        pred_te_rels = core_ent_prediction[doc.name]\n",
    "\n",
    "        # evaluate the document\n",
    "        doc_eval[doc.name] = ta_scorer.evaluate_relations(ref_te_rels, pred_te_rels)\n",
    "\n",
    "    # summarize evaluation of the dataset (all documents)\n",
    "    res = ta_scorer.summarize()\n",
    "\n",
    "    precision = res[\"precision\"]\n",
    "    recall = res[\"recall\"]\n",
    "    temporal_awareness = res[\"fscore\"]\n",
    "    print(f\"{precision:.4f} & {recall:.4f} & {temporal_awareness:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d4a93",
   "metadata": {},
   "source": [
    "should be\n",
    "```\n",
    "'0.3923 & 0.3895 & 0.3909'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a88e90a",
   "metadata": {},
   "source": [
    "##### Temporal Entity Alignment\n",
    "\n",
    "We are running the evaluation metric 5 times with random order annotation ordering to demonstrate robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b57a0354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:52.618147Z",
     "start_time": "2024-01-12T07:43:51.409314Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Entity Alignment\n",
      "Run: 0\n",
      "0.3923 & 0.3260 & 0.3561\n",
      "doc affected by violations: 0\n",
      "\n",
      "Run: 1\n",
      "0.3923 & 0.3260 & 0.3561\n",
      "doc affected by violations: 0\n",
      "\n",
      "Run: 2\n",
      "0.3923 & 0.3260 & 0.3561\n",
      "doc affected by violations: 0\n",
      "\n",
      "Run: 3\n",
      "0.3923 & 0.3260 & 0.3561\n",
      "doc affected by violations: 0\n",
      "\n",
      "Run: 4\n",
      "0.3923 & 0.3260 & 0.3561\n",
      "doc affected by violations: 0\n",
      "\n",
      "total docs: 20\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from temporal_extract.scorer.temporal_alignment import TemporalEntityAlignmentScorer\n",
    "\n",
    "\n",
    "print(\"Temporal Entity Alignment\")\n",
    "\n",
    "for i in range(5):\n",
    "    tea_scorer = TemporalEntityAlignmentScorer()\n",
    "\n",
    "    # store evaluation of each document (used for futher analysis)\n",
    "    doc_eval = {}\n",
    "\n",
    "    for idx, doc in enumerate(te3_ordered.test):\n",
    "        # iterate over each document\n",
    "\n",
    "        # convert reference relations\n",
    "        ref_te_rels = tieval_rels_2_te_rels(doc.tlinks)\n",
    "\n",
    "        pred_te_rels = list(core_ent_prediction[doc.name])\n",
    "        \n",
    "        # shuffle ref and pred to demonstrate our evaluation robustness\n",
    "        # random.shuffle(ref_te_rels)\n",
    "        # random.shuffle(pred_te_rels)\n",
    "\n",
    "        # evaluate the document\n",
    "        doc_eval[doc.name] = tea_scorer.evaluate_relations(ref_te_rels, pred_te_rels)\n",
    "\n",
    "    # summarize evaluation of the dataset (all documents)\n",
    "    res = tea_scorer.summarize()\n",
    "\n",
    "    precision = res[\"precision\"]\n",
    "    recall = res[\"recall\"]\n",
    "    temporal_awareness = res[\"fscore\"]\n",
    "    \n",
    "    print(\"Run:\", i)\n",
    "    print(f\"{precision:.4f} & {recall:.4f} & {temporal_awareness:.4f}\")\n",
    "    print(f\"doc affected by violations: {tea_scorer.model_doc_violation_count}\")\n",
    "    print()\n",
    "    \n",
    "print(f\"total docs: {len(te3.test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af2b60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T09:58:29.043889Z",
     "start_time": "2024-01-06T09:58:29.030198Z"
    }
   },
   "source": [
    "every run should be **approximately**\n",
    "\n",
    "```\n",
    "0.3923 & 0.3260 & 0.3561\n",
    "0.3923 & 0.3248 & 0.3554\n",
    "doc affected by violations: 0\n",
    "\n",
    "total docs: 20\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e796832",
   "metadata": {},
   "source": [
    "##### Temporal Point Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "957ff124",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:52.995758Z",
     "start_time": "2024-01-12T07:43:52.619158Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Point Alignment\n",
      "Run: 0\n",
      "0.3923 & 0.4197 & 0.4055\n",
      "potins affected by violations: 0\n",
      "total points: 808\n",
      "\n",
      "Run: 1\n",
      "0.3923 & 0.4197 & 0.4055\n",
      "potins affected by violations: 0\n",
      "total points: 808\n",
      "\n",
      "Run: 2\n",
      "0.3923 & 0.4197 & 0.4055\n",
      "potins affected by violations: 0\n",
      "total points: 808\n",
      "\n",
      "Run: 3\n",
      "0.3923 & 0.4197 & 0.4055\n",
      "potins affected by violations: 0\n",
      "total points: 808\n",
      "\n",
      "Run: 4\n",
      "0.3923 & 0.4197 & 0.4055\n",
      "potins affected by violations: 0\n",
      "total points: 808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from temporal_extract.scorer.temporal_alignment import TemporalPointAlignmentScorer\n",
    "\n",
    "\n",
    "print(\"Temporal Point Alignment\")\n",
    "\n",
    "for i in range(5):\n",
    "    tpa_scorer = TemporalPointAlignmentScorer()\n",
    "\n",
    "    # store evaluation of each document (used for futher analysis)\n",
    "    doc_eval = {}\n",
    "\n",
    "    for idx, doc in enumerate(te3_ordered.test):\n",
    "        # iterate over each document\n",
    "\n",
    "        # convert reference relations\n",
    "        ref_te_rels = tieval_rels_2_te_rels(doc.tlinks)\n",
    "        \n",
    "        pred_te_rels = list(core_ent_prediction[doc.name])\n",
    "        \n",
    "        # shuffle ref and pred to demonstrate our evaluation robustness\n",
    "        random.shuffle(ref_te_rels)\n",
    "        random.shuffle(pred_te_rels)\n",
    "\n",
    "        # evaluate the document\n",
    "        doc_eval[doc.name] = tpa_scorer.evaluate_relations(ref_te_rels, pred_te_rels)\n",
    "\n",
    "    # summarize evaluation of the dataset (all documents)\n",
    "    res = tpa_scorer.summarize()\n",
    "\n",
    "    precision = res[\"precision\"]\n",
    "    recall = res[\"recall\"]\n",
    "    temporal_awareness = res[\"fscore\"]\n",
    "    \n",
    "    print(\"Run:\", i)\n",
    "    print(f\"{precision:.4f} & {recall:.4f} & {temporal_awareness:.4f}\")\n",
    "    print(f\"potins affected by violations: {tpa_scorer.pt_affected}\")\n",
    "    print(f\"total points: {tpa_scorer.pt_total}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8008e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T09:58:38.683263Z",
     "start_time": "2024-01-06T09:58:38.674764Z"
    }
   },
   "source": [
    "should be\n",
    "\n",
    "```\n",
    "0.3923 & 0.4197 & 0.4055\n",
    "potins affected by violations: 0\n",
    "total points: 808\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c65db",
   "metadata": {},
   "source": [
    "## Temporal Awareness Bug\n",
    "\n",
    "This section clovers how to reproduce the bug we found in [UzZaman's Temporal Awareness evaluator](https://github.com/naushadzaman/tempeval3_toolkit).\n",
    "\n",
    "<img src=\"bug/closure-bug.png\" alt=\"diagram of closure bug\" style=\"width:50%;\"/>\n",
    "\n",
    "We have prepaired three sets of TimeML annotation which contain relations shown in the image above.\n",
    "\n",
    "1. `bug/ref-1`\n",
    "    * the relations r0-r5 are written in order of the relation name (r0, r1, r2, r3, r4, r5)\n",
    "2. `bug/ref-2`\n",
    "    * the relations r0-r5 are written in a different order (r0, r1, r2, r3, **r5, r4**)\n",
    "3. `bug/pred`\n",
    "    * only the relation r6 are written\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5244a87",
   "metadata": {},
   "source": [
    "Ideally, a closure graph should be able to infer that \n",
    "\n",
    "> \"e2 BEFORE e7\" (r6)\n",
    "\n",
    "since\n",
    "\n",
    "> \"e2 BEFORE e3\" (r5) and \"e3 BEFORE e7\" (r3)\n",
    "\n",
    "However, [UzZaman's Temporal Awareness evaluator](https://github.com/naushadzaman/tempeval3_toolkit) is unable to infer \"r6\", as seen in the results bellow.\n",
    "\n",
    "When evaluating `bug/ref-1` against `bug/pred`, we get precision is 0.0, which means that no relation in `bug/pred` (i.e., r6) does not match any of the relations in `bug/ref-1`. This is incorrect because `bug/ref-1` does contain r5 and r3.\n",
    "\n",
    "This bug is hard to detect, see how changing the relations order (i.e., `bug/ref-2`) affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbb06a11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:52.998601Z",
     "start_time": "2024-01-12T07:43:52.996660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python repos/tempeval3_toolkit/evaluation-relations/temporal_evaluation.py \\\n",
      "    temporal_alignment/bug/ref-1 \\\n",
      "    temporal_alignment/bug/pred \\\n",
      "    1 > temporal_alignment/bug/ref-1-pred-eval.txt\n",
      "\n",
      "python repos/tempeval3_toolkit/evaluation-relations/temporal_evaluation.py \\\n",
      "    temporal_alignment/bug/ref-2 \\\n",
      "    temporal_alignment/bug/pred \\\n",
      "    1 > temporal_alignment/bug/ref-2-pred-eval.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ta_script_path = f\"{path_to_tempeval3_toolkit_repo}/evaluation-relations/temporal_evaluation.py\"\n",
    "output_prefix = f\"output/tempeval3_toolkit/eval-ta-{infer_pass}\"\n",
    "print(f\"\"\"python {ta_script_path} \\\\\n",
    "    {path_to_this_repo}/bug/ref-1 \\\\\n",
    "    {path_to_this_repo}/bug/pred \\\\\n",
    "    1 > {path_to_this_repo}/bug/ref-1-pred-eval.txt\n",
    "\n",
    "python {ta_script_path} \\\\\n",
    "    {path_to_this_repo}/bug/ref-2 \\\\\n",
    "    {path_to_this_repo}/bug/pred \\\\\n",
    "    1 > {path_to_this_repo}/bug/ref-2-pred-eval.txt\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf0111dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:53.038971Z",
     "start_time": "2024-01-12T07:43:52.999274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Score\tF1\tP\tR\n",
      "\t\t0.0\t0.0\t0.0\t\n",
      "Overall Temporal Awareness Score (F1 score): 0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.0000 & 0.0000 & 0.0000'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_scores(f\"bug/ref-1-pred-eval.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3822f454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T07:43:53.068556Z",
     "start_time": "2024-01-12T07:43:53.040891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Score\tF1\tP\tR\n",
      "\t\t0.0\t100.0\t0.0\t\n",
      "Overall Temporal Awareness Score (F1 score): 0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.0000 & 0.0000 & 0.0000'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_scores(f\"bug/ref-2-pred-eval.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b45588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "311.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
